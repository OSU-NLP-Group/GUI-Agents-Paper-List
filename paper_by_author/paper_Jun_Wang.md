# Jun Wang's Papers

- [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
    - Ziyun Zhang, Xinyi Liu, Xiaoyi Zhang, Jun Wang, Gang Chen, Yan Lu
    - ğŸ›ï¸ Institutions: Peking University, Microsoft Research Asia
    - ğŸ“… Date: May 28, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [desktop]
    - ğŸ”‘ Key: [framework], [knowledge-execution gap], [Critique Stage], [Retrace Stage], [OSWorld], [selfâ€‘evolution]
    - ğŸ“– TLDR: The paper identifies a â€œknowledgeâ€‘execution gapâ€ where even highly accurate external knowledge (90%) only yields a 41% execution success rate. To bridge this, the authors introduce **UIâ€‘Evol**, a plugâ€‘andâ€‘play twoâ€‘stage module for GUI agentsâ€”**Retrace** recovers actual action sequences from real agentâ€“environment interactions, and **Critique** refines knowledge by comparing these sequences with external references. Experiments using Agent S2 on the OSWorld benchmark show significant gains in task performance and reduced behavioral variance, improving agent reliability.

- [VSC-RL: Advancing Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2502.07949)
    - Qingyuan Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
    - ğŸ›ï¸ Institutions: Huawei Noah's Ark Lab, Univ. College London
    - ğŸ“… Date: February 11, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Mobile]
    - ğŸ”‘ Key: [framework], [reinforcement learning], [subgoal generation], [VSC-RL], [learning efficiency]
    - ğŸ“– TLDR: This paper introduces **VSC-RL**, a novel reinforcement learning framework that enhances learning efficiency for vision-language agents in complex sequential decision-making tasks. By reformulating these tasks as variational goal-conditioned problems, VSC-RL leverages advanced optimization techniques and utilizes vision-language models to autonomously decompose goals into feasible subgoals. Empirical results demonstrate that VSC-RL significantly outperforms state-of-the-art agents, particularly in challenging mobile device control tasks.

- [AppVLM: A Lightweight Vision Language Model for Online App Control](https://arxiv.org/abs/2502.06395)
    - Georgios Papoudakis, Thomas Coste, Zhihao Wu, Jianye Hao, Jun Wang, Kun Shao
    - ğŸ›ï¸ Institutions: Univ. of Cambridge, Huawei Noah's Ark Lab, Univ. College London
    - ğŸ“… Date: February 10, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Mobile]
    - ğŸ”‘ Key: [model], [framework], [evaluation], [AppVLM], [on-device control]
    - ğŸ“– TLDR: This paper introduces **AppVLM**, a lightweight Vision-Language Model designed for efficient on-device control of smartphone applications. AppVLM is fine-tuned on the AndroidControl dataset and further refined through interactions within the AndroidWorld environment. It achieves superior action prediction accuracy in offline evaluations and matches GPT-4o in online task completion success rates, while operating up to ten times faster, making it a practical solution for real-world deployment.

- [Lightweight Neural App Control](https://arxiv.org/abs/2410.17883)
    - Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, Kun Shao
    - ğŸ›ï¸ Institutions: Huawei Noah's Ark Lab, UCL
    - ğŸ“… Date: October 23, 2024
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Mobile]
    - ğŸ”‘ Key: [framework], [vision language model], [Action Transformer], [app agent], [Android control], [multi-modal]
    - ğŸ“– TLDR: This paper introduces LiMAC, a mobile control framework for Android that integrates an Action Transformer and fine-tuned vision-language models to execute precise actions in mobile apps. Tested on open-source datasets, LiMAC improves action accuracy by up to 42% over traditional prompt engineering baselines, demonstrating enhanced efficiency and accuracy in mobile app control tasks.

- [SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation](https://ai-agents-2030.github.io/SPA-Bench/)
    - Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, Rui Shao, Liqiang Nie, Yasheng Wang, Jianye Hao, Jun Wang, Kun Shao
    - ğŸ›ï¸ Institutions: Huawei Noahâ€™s Ark Lab, Harbin Institute of Technology, Shenzhen, UCL
    - ğŸ“… Date: October 19, 2024
    - ğŸ“‘ Publisher: ICLR 2025
    - ğŸ’» Env: [Mobile]
    - ğŸ”‘ Key: [benchmark], [AI agent], [smartphone control], [framework]
    - ğŸ“– TLDR: SPA-Bench is introduced as a benchmark designed to evaluate multimodal large language model (MLLM)-based smartphone agents, offering a task set that spans common smartphone functionalities across system and third-party applications. It includes a plug-and-play framework for real-time agent interactions on Android, integrating over ten agents with an adaptable evaluation pipeline measuring success across diverse metrics. Through this, the benchmark exposes challenges such as UI interpretation, action grounding, and memory retention in mobile environments, advancing research in smartphone-based agent applications.

- [DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents](https://ai-agents-2030.github.io/DistRL/)
    - Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
    - ğŸ›ï¸ Institutions: Univ. of Cambridge, Huawei Noah's Ark Lab, Univ. College London
    - ğŸ“… Date: October 18, 2024
    - ğŸ“‘ Publisher: ICLR 2025
    - ğŸ’» Env: [Mobile]
    - ğŸ”‘ Key: [framework], [reinforcement learning], [distributed training], [A-RIDE], [on-device control]
    - ğŸ“– TLDR: This paper introduces **DistRL**, a novel framework designed to enhance the efficiency of online reinforcement learning fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in dynamic online interactions. The framework is supported by a custom RL algorithm, A-RIDE, which balances exploration with prioritized data utilization to ensure stable and robust training. Experiments demonstrate that DistRL improves training efficiency by 3Ã— and accelerates data collection by 2.4Ã— compared to leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate on general Android tasks from an open benchmark, outperforming existing approaches while maintaining the same training time.
