# Caiming Xiong's Papers

- [GUIâ€‘KV: Efficient GUI Agents via KV Cache with Spatioâ€‘Temporal Awareness](https://arxiv.org/abs/2510.00536)
    - Kungâ€‘Hsiang Huang, Haoyi Qiu, Yutong Dai, Caiming Xiong, Chienâ€‘Sheng Wu
    - ğŸ›ï¸ Institutions: Salesforce AI Research, UCLA
    - ğŸ“… Date: October 1, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [framework], [KV cache compression], [spatial saliency], [temporal redundancy], [plugâ€‘andâ€‘play]
    - ğŸ“– TLDR: This paper proposes GUI-KV, a plug-and-play KV cache compression method for GUI agents, which improves efficiency by combining spatial saliency-guided token selection and temporal redundancy pruning. It significantly reduces memory and computation costs while maintaining or even surpassing original model performance across multiple GUI agent benchmarks.

- [SCUBA: Salesforce Computer Use Benchmark](https://sfrcua.github.io/SCUBA/)
    - Yutong Dai, Krithika Ramakrishnan, Jing Gu, Matthew Fernandez, Yanqi Luo, Viraj Prabhu, Zhenyu Hu, Silvio Savarese, Caiming Xiong, Zeyuan Chen, Ran Xu
    - ğŸ›ï¸ Institutions: Salesforce
    - ğŸ“… Date: Sep 30, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [benchmark], [enterprise automation], [demonstration augmentation], [process rewards], [SCUBA]
    - ğŸ“– TLDR: This paper introduces **SCUBA**, a benchmark for evaluating computer-use agents on 300 enterprise-critical CRM tasks within Salesforce. Tasks span personas like admins and service agents, testing UI navigation, automation, data handling, and troubleshooting. SCUBA includes sandbox environments, milestone-based evaluations, and supports parallel execution. Baseline results show large performance gaps across agent designs, especially between open- and closed-source models, with success rates ranging from 5% to 50% depending on demonstration usage.

- [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
    - Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li
    - ğŸ›ï¸ Institutions: Salesforce AI Research
    - ğŸ“… Date: August 20, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Misc]
    - ğŸ”‘ Key: [benchmark], [dataset], [framework], [long-horizon reasoning], [unknown-tools challenge], [execution-based evaluation], [MCP-Universe]
    - ğŸ“– TLDR: MCP-Universe introduces the first comprehensive benchmark for evaluating large language models (LLMs) through interactions with real-world Model Context Protocol (MCP) servers. It spans six core domainsâ€”Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searchingâ€”across 11 MCP servers. The benchmark employs execution-based evaluators (format, static, dynamic) to rigorously assess agent performance. Despite progress, state-of-the-art models like GPT-5 (43.72% success), Grok-4 (33.33%), and Claude-4.0-Sonnet (29.44%) show significant limitations. The benchmark highlights challenges in long-context reasoning and unfamiliar tool handling, and provides an open-source extensible evaluation framework with UI support to accelerate future research. 

- [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
    - Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Silvio Savarese, Caiming Xiong, Junnan Li
    - ğŸ›ï¸ Institutions: Salesforce AI Research, ANU, HKU
    - ğŸ“… Date: July 8, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [test-time scaling], [reinforcement learning], [planning], [grounding], [GTA1]
    - ğŸ“– TLDR: This paper tackles two major challenges in GUI agents â€” planning ambiguity and visual grounding accuracy. It introduces **GTA1**, a GUI Test-time Scaling Agent that improves action decision-making by sampling multiple candidate actions at each step and selecting the best one via a judge model. Additionally, it enhances grounding through reinforcement learning with success-based rewards. GTA1 achieves state-of-the-art results on GUI grounding and execution benchmarks such as ScreenSpot and OSWorld.

- [Scaling Computerâ€‘Use Grounding via User Interface Decomposition and Synthesis](https://osworld-grounding.github.io/)
    - Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, Caiming Xiong
    - ğŸ›ï¸ Institutions: The University of Hong Kong, Salesforce AI Research
    - ğŸ“… Date: MayÂ 19,Â 2025
    - ğŸ“‘ Publisher: arXiv (preprint)
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [dataset], [benchmark], [framework], [Jedi], [OSWorldâ€‘G], [GUI grounding], [compositional generalization], [fineâ€‘grained manipulation]
    - ğŸ“– TLDR: This work tackles limitations in GUI grounding by introducing **OSWorldâ€‘G**, a 564â€‘sample benchmark covering text matching, element recognition, layout understanding, fineâ€‘grained manipulation, and refusal detection. They also synthesize **Jedi**, a massive 4â€¯Mâ€‘example dataset generated via UI decomposition and synthesis. Training multiâ€‘scale models on Jedi achieves stateâ€‘ofâ€‘theâ€‘art grounding across benchmarks (ScreenSpotâ€‘v2/Pro and OSWorldâ€‘G), boosting agentic task performance from 5â€¯% to ~27â€¯% success. Ablations demonstrate compositional benefits. All artifacts are publicly released.

- [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://aguvis-project.github.io/)
    - Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong
    - ğŸ›ï¸ Institutions: HKU, NTU, Salesforce
    - ğŸ“… Date: Dec 5, 2024
    - ğŸ“‘ Publisher: ICML 2025
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [model], [dataset], [planning], [reasoning], [Aguvis], [visual grounding]
    - ğŸ“– TLDR: This paper introduces *Aguvis*, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. It leverages image-based observations and grounds natural language instructions to visual elements, employing a consistent action space to ensure cross-platform generalization. The approach integrates explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. A large-scale dataset of GUI agent trajectories is constructed, incorporating multimodal reasoning and grounding. Comprehensive experiments demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. All datasets, models, and training recipes are open-sourced to facilitate future research.

- [Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?](https://spider2-v.github.io/)
    - Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongsheng Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu
    - ğŸ›ï¸ Institutions: HKU, SJTU, Google Cloud AI Research, Google DeepMind, Salesforce Research, Yale University, Sea AI Lab, University of Waterloo
    - ğŸ“… Date: July 15, 2024
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Desktop]
    - ğŸ”‘ Key: [benchmark], [dataset], [data science], [engineering workflows], [Spider2-V]
    - ğŸ“– TLDR: This paper introduces **Spider2-V**, a multimodal agent benchmark designed to evaluate the capability of agents in automating professional data science and engineering workflows. It comprises 494 real-world tasks across 20 enterprise-level applications, assessing agents' proficiency in code generation and GUI operations within authentic computer environments.

- [OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments](https://arxiv.org/abs/2404.07972)
    - Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu
    - ğŸ›ï¸ Institutions: HKU, CMU, Salesforce, University of Waterloo
    - ğŸ“… Date: April 11, 2024
    - ğŸ“‘ Publisher: NeurIPS 2024
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [benchmark], [real computer tasks], [online environment], [online benchmark]
    - ğŸ“– TLDR: OSWorld introduces a groundbreaking benchmark for multimodal agents to perform open-ended tasks within real computer environments across platforms like Ubuntu, Windows, and macOS. It includes 369 real-world tasks involving web and desktop apps, file management, and multi-app workflows, with custom evaluation scripts for reproducibility. The results reveal current agentsâ€™ limitations in GUI interaction and operational knowledge, as they achieve just 12.24% task success compared to humans' 72.36%, highlighting critical gaps for future model improvement.

- [OpenAgents: An Open Platform for Language Agents in the Wild](https://github.com/xlang-ai/OpenAgents)
    - Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu
    - ğŸ›ï¸ Institutions: HKU, XLang Lab, Sea AI Lab, Salesforce Research
    - ğŸ“… Date: October 16, 2023
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Web]
    - ğŸ”‘ Key: [framework], [Data Agent], [Plugins Agent], [Web Agent]
    - ğŸ“– TLDR: This paper introduces OpenAgents, an open-source platform designed to facilitate the use and hosting of language agents in real-world scenarios. It features three agents: Data Agent for data analysis using Python and SQL, Plugins Agent with access to over 200 daily API tools, and Web Agent for autonomous web browsing. OpenAgents aims to provide a user-friendly web interface for general users and a seamless deployment experience for developers and researchers, promoting the development and evaluation of innovative language agents in practical applications.
