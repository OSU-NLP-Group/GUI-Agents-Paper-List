# Zhiyong Wu's Papers

- [UIâ€‘TARSâ€‘2 Technical Report: Advancing GUI Agent with Multiâ€‘Turn Reinforcement Learning](https://arxiv.org/abs/2509.02544)
    - Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, Wanjun Zhong, Yining Ye, Yujia Qin, Yuwen Xiong, Yuxin Song, Zhiyong Wu, Bo Li, Chen Dun, Chong Liu, Fuxing Leng, Hanbin Wang, Hao Yu, Haobin Chen, Hongyi Guo, Jing Su, Jingjia Huang, Kai Shen, Kaiyu Shi, Lin Yan, Peiyao Zhao, Pengfei Liu, Qinghao Ye, Renjie Zheng, Wayne Xin Zhao, Wen Heng, Wenhao Huang, Wenqian Wang, Xiaobo Qin, Yi Lin, Youbin Wu, Zehui Chen, Zihao Wang, Baoquan Zhong, Xinchun Zhang, Xujing Li, Yuanfan Li, Zhongkai Zhao, Chengquan Jiang, Faming Wu, Haotian Zhou, Jinlinâ€¯Pang, Liâ€¯Han, Qianliâ€¯Ma, Siyaoâ€¯Liu, Songhuaâ€¯Cai, Wenqiâ€¯Fu, Xinâ€¯Liu, Zhiâ€¯Zhang, Boâ€¯Zhou, Guoliangâ€¯Li, Jiajunâ€¯Shi, Jialeâ€¯Yang, Jieâ€¯Tang, Liâ€¯Li, Taoranâ€¯Lu, Woyuâ€¯Lin, Xiaokangâ€¯Tong, Xinyaoâ€¯Li, Yichiâ€¯Zhang, Yuâ€¯Miao, Zhengxuanâ€¯Jiang, Ziliâ€¯Li, Ziyuanâ€¯Zhao, Chenxinâ€¯Li, Dehuaâ€¯Ma, Fengâ€¯Lin, Geâ€¯Zhang, Haihuaâ€¯Yang, Hangyuâ€¯Guo, Hongdaâ€¯Zhu, Jiahengâ€¯Liu, Jundaâ€¯Du, Kaiâ€¯Cai, Kuanyeâ€¯Li, Lichenâ€¯Yuan, Meilanâ€¯Han, Minchaoâ€¯Wang, Shuyueâ€¯Guo, Tianhaoâ€¯Cheng, Xiaoboâ€¯Ma, Xiaojunâ€¯Xiao, Xiaolongâ€¯Huang, Xinjieâ€¯Chen, Yidiâ€¯Du, Yilinâ€¯Chen, Yiwenâ€¯Wang, Zhaojianâ€¯Li, Zhenzhuâ€¯Yang, Zhiyuanâ€¯Zeng, Chaolinâ€¯Jin, Chenâ€¯Li, Haoâ€¯Chen, Haoliâ€¯Chen, Jianâ€¯Chen, Qinghaoâ€¯Zhao, Guangâ€¯Shi
    - ğŸ›ï¸ Institutions: ByteDance Seed
    - ğŸ“… Date: September 2, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [model], [reinforcement learning], [UIâ€‘TARSâ€‘2]
    - ğŸ“– TLDR: UIâ€‘TARSâ€‘2 is a newly trained native GUIâ€‘centered agent that uses a scalable data flywheel, stabilized multiâ€‘turn reinforcement learning, hybrid GUI + terminal environments, and a unified sandbox. It achieves leading performance across diverse GUI benchmarks (e.g., 88.2 on Onlineâ€‘Mind2Web), game tasks (~60% human-level), and generalizes to information-seeking and software engineering tasks. Training dynamics and parameter interpolation offer insights into robust, efficient agent RL at scale.

- [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://qiushisun.github.io/OS-Genesis-Home/)
    - Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu
    - ğŸ›ï¸ Institutions: Shanghai AI Lab, HKU, Johns Hopkins University, SJTU, Oxford, HKUST
    - ğŸ“… Date: Dec 27, 2024
    - ğŸ“‘ Publisher: ACL 2025
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [model], [dataset], [trajectory data], [reward model], [e2e model], [data synthesis], [OS-Genesis]
    - ğŸ“– TLDR: This paper introduces *OS-Genesis*, an interaction-driven pipeline that automates the construction of high-quality and diverse GUI agent trajectory data without human supervision. By employing reverse task synthesis and a trajectory reward model, OS-Genesis enables effective end-to-end training of GUI agents, significantly enhancing their performance on online benchmarks.

- [OS-ATLAS: A Foundation Action Model For Generalist GUI Agents](https://osatlas.github.io/)
    - Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, Yu Qiao
    - ğŸ›ï¸ Institutions: Shanghai AI Lab, SJTU, HKU, MIT
    - ğŸ“… Date: October 30, 2024
    - ğŸ“‘ Publisher: ICLR 2025
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [model], [dataset], [benchmark], [OS-Atlas]
    - ğŸ“– TLDR: This paper introduces OS-Atlas, a foundational GUI action model designed to enhance GUI grounding and out-of-distribution tasks. The authors developed a toolkit to synthesize multi-platform GUI grounding data, resulting in a cross-platform corpus of over 13 million GUI elements. OS-Atlas demonstrates significant performance improvements across six benchmarks spanning mobile, desktop, and web platforms.

- [AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant](https://arxiv.org/abs/2410.18603)
    - Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, Zhiyong Wu
    - ğŸ›ï¸ Institutions: XJTU, Shanghai AI Lab, HKU
    - ğŸ“… Date: October 24, 2024
    - ğŸ“‘ Publisher: ACL 2025
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [framework], [multi-agent systems], [specialized generalist agent], [OSWorld benchmark]
    - ğŸ“– TLDR: AgentStore introduces a scalable platform to integrate and manage heterogeneous agents, designed to enhance generalist assistant capabilities for diverse computer tasks. Using a MetaAgent and AgentToken strategy, AgentStore shows improved generalization on the OSWorld benchmark.

- [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456)
    - Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong
    - ğŸ›ï¸ Institutions: Shanghai AI Lab, East China Normal University, Princeton, HKU
    - ğŸ“… Date: February 12, 2024
    - ğŸ“‘ Publisher: ICLR 2024 Workshop LLMAgents
    - ğŸ’» Env: [Desktop]
    - ğŸ”‘ Key: [framework], [self-directed learning], [GAIA], [FRIDAY], [OS-Copilot]
    - ğŸ“– TLDR: The OS-Copilot framework supports building generalist agents capable of performing diverse tasks across an operating system (OS). This work introduces FRIDAY, an embodied agent using OS-Copilot to self-improve by learning from task outcomes. It operates with a memory-based architecture to tackle OS-level tasks across applications like terminals, web browsers, and third-party tools. Tested on the GAIA benchmark, FRIDAY achieved 35% higher performance than prior methods, proving effective in adapting to unfamiliar applications and refining its capabilities with minimal guidance.

- [SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents](https://arxiv.org/abs/2401.10935)
    - Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, Zhiyong Wu
    - ğŸ›ï¸ Institutions: Nanjing University, Shanghai AI Lab
    - ğŸ“… Date: January 19, 2024
    - ğŸ“‘ Publisher: ACL 2024
    - ğŸ’» Env: [GUI]
    - ğŸ”‘ Key: [model], [benchmark], [GUI grounding], [visual grounding]
    - ğŸ“– TLDR: TBD.

- [Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](https://arxiv.org/abs/2406.11736)
    - Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu
    - ğŸ›ï¸ Institutions: Xi'an Jiaotong University, Shanghai AI Lab, HKU, Nanjing University
    - ğŸ“… Date: November 2023
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [GUI (evaluated on web, math reasoning, and logic reasoning environments)]
    - ğŸ”‘ Key: [framework], [dataset], [neural-symbolic self-training], [online exploration], [self-refinement]
    - ğŸ“– TLDR: This paper introduces *ENVISIONS*, a neural-symbolic self-training framework designed to improve large language models (LLMs) by enabling self-training through interaction with a symbolic environment. The framework addresses symbolic data scarcity and enhances LLMs' symbolic reasoning proficiency by iteratively exploring, refining, and learning from symbolic tasks without reinforcement learning. Extensive evaluations across web navigation, math, and logical reasoning tasks highlight *ENVISIONS* as a promising approach for enhancing LLM symbolic processing.
